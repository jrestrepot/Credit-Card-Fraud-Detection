# -*- coding: utf-8 -*-
"""Machine_Learning_ Basics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18KdcCsOO9BfOT2Q4zTh_D56VSnlysgFJ

# Machine Learning Basics
### *Semillero Inteligencia Artificial*


**Asesor: Santiago Hincapie**

**Kaggle Datasets**

https://www.kaggle.com/mlg-ulb/creditcardfraud

https://www.kaggle.com/sbhatti/financial-sentiment-analysis

https://www.kaggle.com/andrewmvd/cyberbullying-classification

**Coursera Guides**

https://www.coursera.org/learn/machine-learning

# Credit Card Fraud Detection
Dealing with an Umbalanced Dataset

### Download Data
"""

! wget https://github.com/coberndorm/SemilleroIA/blob/main/Data.zip?raw=true
! unzip Data.zip?raw=true

!pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download mlg-ulb/creditcardfraud
! unzip /content/creditcardfraud.zip

"""## Exploratory Data Analysis"""

import pandas as pd
import seaborn as sns
import numpy as np
import sklearn as skl
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, RobustScaler

df = pd.read_csv('/content/creditcard.csv')

# Preliminary revision of data
df.head()

#Description of the columns
df.describe()

# No missing or empty data
df.isna().sum().sum()

df['Class'].value_counts()

# Extremely High Skewedness
print('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')
print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')

def Distribution_plot(df):
    amount_val = df['Amount'].values
    time_val = df['Time'].values
    v1_val = df['V1'].values
    v2_val = df['V2'].values
    v3_val = df['V3'].values

    fig, ax = plt.subplots(1, 5, figsize=(18,4))

    sns.histplot(amount_val, ax=ax[0], color='r')
    ax[0].set_title('Distribution of Transaction Amount', fontsize=14)
    ax[0].set_xlim([0, 100])

    sns.histplot(time_val, ax=ax[1], color='b')
    ax[1].set_title('Distribution of Transaction Time', fontsize=14)
    ax[1].set_xlim([min(time_val), max(time_val)])

    sns.histplot(v1_val, ax=ax[2], color='m')
    ax[2].set_title('Distribution of V1', fontsize=14)
    ax[2].set_xlim([min(v1_val), max(v1_val)])

    sns.histplot(v3_val, ax=ax[3], color='g')
    ax[3].set_title('Distribution of V2', fontsize=14)
    ax[3].set_xlim([min(v2_val), max(v2_val)])

    sns.histplot(v1_val, ax=ax[4], color='c')
    ax[4].set_title('Distribution of V3', fontsize=14)
    ax[4].set_xlim([min(v3_val), max(v3_val)])

    plt.show()

Distribution_plot(df)

"""**Data Scaling**"""

from imblearn.under_sampling import NearMiss
from sklearn.preprocessing import MinMaxScaler

"""Min Max Scaling"""

columnames = list(df.columns)
scaler = MinMaxScaler()

scaled_df = pd.DataFrame(scaler.fit_transform(df), columns = columnames )

"""**Outlier Detection**"""

from scipy.spatial.distance import mahalanobis
from scipy.stats import chi2

n_columns = df.shape[1]
n_rows = df.shape[0]
p70r = n_columns*0.60
df = df.dropna(axis=0,thresh=p70r)
df = df.fillna(method = 'bfill') 

def mahalanobis_distances(df, axis=0):
    '''
    Returns a pandas Series with Mahalanobis distances for each sample on the
    axis.

    Note: does not work well when # of observations < # of dimensions
    Will either return NaN in answer
    or (in the extreme case) fail with a Singular Matrix LinAlgError

    Args:
        df: pandas DataFrame with columns to run diagnostics on
        axis: 0 to find outlier rows, 1 to find outlier columns
    '''
    df = df.transpose() if axis == 1 else df
    means = df.mean()
    try:
        inv_cov = np.linalg.inv(df.cov())
    except LinAlgError:
        return pd.Series([np.NAN] * len(df.index), df.index,
                         name='Mahalanobis')
    dists = []
    for i, sample in df.iterrows():
        dists.append(mahalanobis(sample, means, inv_cov))

    return pd.Series(dists, df.index, name='Mahalanobis') 
    
mahalanobis = mahalanobis_distances(scaled_df, axis = 0)
df_Mahal = pd.concat([scaled_df,mahalanobis], axis = 1)
df_Mahal

cutoff = chi2.ppf(0.95, df_Mahal.shape[1])
outlierIndexes = np.where(df_Mahal.loc[:,'Mahalanobis'] > cutoff )
print(outlierIndexes)
print(df_Mahal.shape[0])
print(df_Mahal.loc[:,'Class'].sum())
indexes_to_keep = set(range(df.shape[0])) - set(outlierIndexes[0])
df_cutoff = df.take(list(indexes_to_keep))
print(df_cutoff.loc[:,'Class'].sum())

"""**Undersamplig**"""

undersample = NearMiss(version=1)

X = df.loc[:,'Time':'Amount']
y = df.loc[:,'Class']
X, y = undersample.fit_resample(X, y)

new_df = pd.concat([X,y], axis=1)
new_df

"""**Normal Equation** \
Do Normal solution
Check if running it multiple times and taking the average gives a better solution, this works for lineal regression
Analytical solution to theta

"""

from sklearn import metrics

def find_theta(X, y):
    
    m = X.shape[0] # Number of training examples. 
    # Appending a cloumn of ones in X to add the bias term.
    X = np.append(X, np.ones((m,1)), axis=1)    
    # reshaping y to (m,1)
    y = y.reshape(m,1)
    
    # The Normal Equation
    theta = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))
    
    return theta

def predict_linearRegression(X):
    
    # Appending a cloumn of ones in X to add the bias term.
    X = np.append(X, np.ones((X.shape[0],1)), axis=1)
    
    # preds is y_hat which is the dot product of X and theta.
    preds = np.dot(X, theta)

    return preds

# Getting the Value of theta using the find_theta function.
theta = find_theta(X = X.to_numpy(), y = y.to_numpy())

# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.

# Lets shuffle the data before creating the subsamples

df_test = df.sample(frac=1)

# amount of fraud classes 492 rows.
fraud_df = df_test.loc[df['Class'] == 1]
non_fraud_df = df_test.loc[df['Class'] == 0][:492]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)

new_df

X = new_df.loc[:,'Time':'Amount']
y = new_df.loc[:,'Class']

# Getting the predictions on X using the predict function.
preds = predict_linearRegression(X)

fpr, tpr, threshold = metrics.roc_curve(y,preds)

roc_auc = metrics.auc(fpr, tpr)

print(roc_auc)

"""Data relatively lineally separable, dimension reduction, clustering or dropping data may improve prediction"""